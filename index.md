---
layout: default
---

## About Me

<img class="profile-picture" src="{{ site.baseurl }}/assets/profile_picture.jpg">

I'm Frederik! I'm a B.Sc. in computer science and philosophy from Roskilde University and M.Sc. in IT & Cognition from Copenhagen University. I now work as an external lecturer and project supervisor at Roskilde University's computer science department. In addition to experience in software development, I'm a member of the art- and research based collective [multivocal](http://www.multivocal.org) and a co-initiator of [Group Therapy](https://www.facebook.com/grouptherapy.cph/), a string of parties in Copenhagen exploring diversity and community in and around the dj booth.

Below is a selection of physical, digital and academic work that I've been or currently am involved in.
<br/> <br/>

## HandPose OSC

![HandPose OSC screenshot]({{ site.baseurl }}/assets/handpose-osc.gif "HandPose OSC")
*[HandPose OSC](https://github.com/faaip/HandPose-OSC/releases) screen capture*<br/><br/>

[HandPose OSC](https://github.com/faaip/HandPose-OSC) is a standalone program, which runs the MediaPipe HandPose model on a webcam input and outputs the predictions as OSC. This makes this app useful for use in hand-based interaction design. As of v1.1, it's available to [download](https://github.com/faaip/HandPose-OSC/releases) for osx, linux and windows.

## Lyden af Struer

![Lyden Af Struer screenshot]({{ site.baseurl }}/assets/lydenafstruer.png "Lyden Af Struer")
*[Lyden Af Struer](http://www.lydenafstruer.dk) screenshot*<br/><br/>

"Lyden Af Struer" is a web site for documenting and exploring sound in the city of Struer. On this website users could both record and play sounds of the city. This was done in order to put focus on the invisible but audible aspects of a city.
The platform was created by the Interactive Spaces Lab, where I was a co-developer on the project. The project can be seen [here](http://www.lydenafstruer.dk).

## Signal & Noise (MIT Media Lab)

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://player.vimeo.com/video/288415860' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div>
*Video demonstration of the created work*<br/><br/>

[Signal & Noise](https://www.media.mit.edu/events/mlberlin-signalandnoise/) was a week-long workshop hosted in Berlin by the MIT Media Lab.
I was here a part of the track called "Machine Learning for Creative AI".
As a part of an inter-disciplinary team, I created a work which consisted of a reinforcement learning agent playing the classic atari game "Pong".
The performance of the agent and goals of the agent was affected by whether or not a human observe the training. This was done in order to explore the power structure in relation to machine learning engineers and anthropomorphic ideas of values within a virtual environment.
Does the AI really care about the outcome of it's mindless tasks?

This was created in collaboration with Julian Tapales, Angela Barriga and Maria Han.

## Gloria Stage (Roskilde Festival)

![Gloria Stage]({{ site.baseurl }}/assets/gloria.jpg "Gloria Stage - Roskilde '18")
*Gloria Stage at Roskilde Festival '18 - Photo by Mariliis Kundla*<br/><br/>

At Roskilde Festival 2018, I was a part of the team that helped build the Gloria stage and develop software for the visuals. This was done alongside Vertigo (formerly Obscura) and a bunch of talented visual artists, scene technicians and designers.
My main role here was to develop control tools for use in production, allowing artists and VJ's to focus on the visual experience itself. 


## Digital Neighbourhood

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://player.vimeo.com/video/286516638' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div>
*Video demonstration by Interactive Spaces Lab*<br/><br/>

Digitale Bydele (Digital Neighbourhood) is a collaboration between the Alexandra Institute and the municipality of Aarhus.
In consists of several "phone booths", where citizens are able to record ideas and give feedback to the municipality. This is done in order to create connections with citizens who might be involved in more common forms of citizens engagement.
By using a phone booth with an attached screen, we utilized an artifact already familiar to most users but used in a novel and engaging way.
For this project, I developed the front-end of the software, which was a multi-platform (OSX and Linux) application running in C++.


## The Wave

![The Wave photo]({{ site.baseurl }}/assets/thewave.jpeg "The Wave")
*The Wave (Photo credit: Paul Grover for The Telegraph)*<br/><br/>
The Wave is a large-scale interactive light and sound installation created by the Danish company [Vertigo](https://www.vertigo.dk/). It consists of forty triangular gates that light up and produce sound.
I worked as a developer on the software elements that consisted of tracking users within the installation and creating the effect of a POV moving towards the users as they traversed through the gates.
This was done in OpenFrameworks, python and Max Msp.

## multivocal

![multivocal logo]({{ site.baseurl }}/assets/multivocal_logo.svg "multivocal logo")
Multivocal is an art- and research-based collective exploring the politics and aesthetics of synthesized voics. Normally synthesized voices are trained by a single voice actor, which only has a single vocal identity. In multivocal we question the aesthetic design as well as the representational modes of these synthesized voices, and ask: since voices from machines are not limited to a single vocal identity, why do the currently available synthesized voices have only one gender, one age, and one accent? Multivocal built a voice recording box, which was used at for Roskilde Festival '17, Techfest, IDA and Catch at Click Festival. Additionally we have created audio works showcased at Norberg Festival radio.

In relation to this I have done talks discussing gender, representation and collective voices at [IDA - Driving IT](https://universe.ida.dk/driving-it/speaker/creating-a-multivocal-synthetic-voice/), [Techfestival](http://techfestival.co), [Copenhagen Maker](http://www.copenhagenmaker.com/lrdagsaturday-99/), the [Techtopia Podcast](http://tv.ida.dk/video/18994693/techtopia-17-the-copenhagen-letter-et-manifest-om?start=1156) (in Danish) and at ["Kulturen på P1"](https://multivocal.org/sound/kulturen_paa_p1_frederik.mp3).

I continued this project as a part of my master's thesis project, where I did several experiments with creating voices using multi-speaker datasets and deep learning approaches to Text-to-speech algorithms.

Multivocal is: Alice Emily Baird, Stina Hasse Jørgensen, Mads Steensig Pelt, Nina Cecilie Højholdt and me.

## Graphic design for anyines

![perma screenshot]({{ site.baseurl }}/assets/perma_screenshot.png "anyines visuals")
*Screenshot from the [Perma website](https://anyines.com/perma/).*<br/><br/>

Anyines is a danish music label started by the two electronic musicians Villads Klint and Aske Zidore. I have created independent web-pages for some of their releases, which are supposed to act as engaging visual counter-points to the music. So far this has been done for the releases for [Minais B.](https://anyines.com/deepcare/) and [An Gella](https://anyines.com/perma/).

## Skyskraber - Skyhearted

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://www.youtube.com/embed/pJj-OcDeW8k/?vq=hd720' frameborder='0' allowfullscreen></iframe></div>

I shot and edited a music video for the Danish band [Skyskraber](https://www.facebook.com/skyskrabermusic/). The main idea was to capture a "live-in-the-studio"-feel, but shoot it using a depth camera and which makes it possible to work with a virtual camera in addition to the physical one. This was done using a simple program I wrote in C++ and was later edited using Premiere Pro and After Effects.

The program requires openFrameworks 0.9.8 and a Kinect camera to run. The repository can be found on [github](https://github.com/faaip/skyskraber).

## TREE.0

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://player.vimeo.com/video/210922192' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div>

TREE.0 is an urban installation placed on Vester Voldgade at the [Copenhagen Street Lab](http://cphsolutionslab.dk). TREE.0 was designed to experiment with using sensor data to create a transforming and adaptable urban experience.

Smart City technologies are often build on the idea of pervasive and hidden technologies. TREE.0 aimed to turn this notion around and create an experience that could act as catalyst for discussions around adaptable environments, smart cities and data collection.

The design is a six metre tall steel tree with LED fixtures, wifi-, weather- and pressure sensors. The light- and soundscape of the Tree morphs and changes based on machine learning algorithm. This algorithm allows for interpolation between different design presets and thereby creates an ever-changing light and sound scenario based on sensor data and usage patterns. This approach allowed for a certain unpredictability, will still being able to build design knowledge from qualitative studies and observations.

The Tree is designed by the Alexandra Institute in collaboration with Obscura/Vertigo and Gehl Architects. My role in this project has been in programming, software and sound design. The software for the Tree is a patch-work of different programs written in C++, Java, Python and Max Msp.

Selected press: [Berlingske](http://www.b.dk/kultur/traeet-der-spiller-naar-du-danser)

<br/> <br/>

## MEETING NAO

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://www.youtube.com/embed/jGsH7Mcso_0' frameborder='0' allowfullscreen></iframe></div>

'Meeting Nao' is a video intended for use in an auto-ethnographic study of Aldebarans famous Nao robot. As a part of the course 'Robots and artificial life' at the University of Copenhagen, I participated in three _meetings_ with Nao during the spring of 2016 with Pernille Zidore Nygaard, Sofie Fogt and Martin Nohns.

I used this video as a catalyst to discuss the appearance of alterity- and hermeneutic relations as they are described in the post-phenomenology of Don Ihde. I here argued that the appearance of Nao as an alterity was often due to black-boxing or lack of understanding of the sub-technologies of the robot. The appearance of hermeneutic layers in the interaction allowed Nao to refer to different behaviors and social contexts, but allowed subroutines programmed by the user.
<br/> <br/>
<br/> <br/>

## BYMODEL

![alt text]({{ site.baseurl }}/assets/Update_1.jpg "Update exhibition")

The city model (bymodel) is an ongoing project at the Interactive Spaces Lab. It's based on the idea of visualizing the invisible digital layers of the city in a playful and engaging way.

The first iteration of the city model was a part of the [Update-exhibition](http://www.dac.dk/da/dac-life/udstillinger/2016/update---goer-byen-smart/) at the Danish Architecture Center. This exhibition focused on smart technologies in cities and discussed big data and it's implications on humans.

The city model consists of 6 high-resolution LED screens under clear armored glass. Users can then walk on the screens and their movement is tracked by two kinect cameras. The content on the screen is icon-based, where the presence of users unveils previously hidden digital information about the city. The following iterations of the city model focused more on visualizing live data on from various sensors in the city. Under the hood this included a major overhaul of the software, where an increased modularity allows for easier integration of various APIs with city data. This newer version was exhibited at Danish Industry and at Smart City Expo in Barcelona.

The city model is developed at the Interactives Spaces Lab in collaboration with Obscura/Vertigo, Jonas Jongejan and Karina Korsgaard. I've been handling the development and programming of the software and also coordinated integration of APIs with various partners.
<br/> <br/>
<br/> <br/>

## The use of evolutionary algorithms in computational design

![alt text]({{ site.baseurl }}/assets/voronoi_1.png "EA Voronoi")
My bachelor thesis explored the use of evolutionary algorithms in design and architecture. In this study we created a software program that created and evolved three-dimensional voronoi structures based on the output an evolutionary algorithm. The user was able to parameterize the algorithm, select fitness function and watch shapes gradually transform. This piece of software was then used to explore the relation between user (subject) and software (technology). In this we concluded that in order to utilize the emergent qualities of an evolutionary algorithm, the subject shifts between two different outer positions: one where the subjects adjusts and interprets the output of the algorithm and another, where the output of the algorithm becomes the objects of fascination based on unexpected emergent qualities. The study received highest marks 12 (A) and was done in collaboration with Sara Daugbjerg and Julie Eg Thøstensen. In this constellation I did most of the software development and phenomenological aspects of the study.

The thesis, while in Danish, can be read [here](pdfs/evolutionary_algorithms.pdf). The source code for the software can be found on [Github](https://github.com/faaip/Evolutionary-Voronoi).
<br/> <br/>
<br/> <br/>

## Wrapcity

![alt text]({{ site.baseurl }}/assets/wrapcity_1.jpg "Wrapcity - black/white")
*Wrapcity* was an installation at Roskilde Festival 2015, which was built using scaffolding and 30+ km of plastic wrap. I joined a multi-disciplinary team put together by the Danish Architecture Center and WrapCity was the outcome - a visual interpretation of the complexity, quantity and interconnectedness of Big Data. This installation was done in a 5 day sprint. Created in collaboration with Caroline Beck, Jakob Franijeur, Ove Bitsch Olsen and Nielsine Otto.

## Magnetic sensing

![alt text]({{ site.baseurl }}/assets/magnet_1.gif "Magnet implant")
*Magnetic implant in ring finger*

During the 3rd semester at Roskilde University, me and my group wanted to explore transhumanism and technological expansion of the senses. It was an intriguing approach to not only do this study from a purely theoretical point, but to include a more subjective element as well. One thing led to another and me and a friend had neodymium magnets implanted in our ring fingers. Magnetic implants allows one to sense a magnetic field and a large part of the project was based on studying and experimenting with the qualia of different magnetic fields. The semester project is called 'Sensory modality in a transhuman perspective' and can be read [here](pdfs/Sensorisk modalitet i et transhumanistisk perspektiv.pdf) (in danish). The project was done in collaboration with Julie Eg Thøstesen, Nis Conrad Nissen, Sebastian Gandsø and Younes Haroun Bakhti and received highest marks 12 (A).

I have later done talks on magnetic sensing at a Pecha Kucha event and on national radio. This interview was later criticized at this [blog](http://www.scienceblog.dk/2016/05/26/soelvpapirshat-om-hvordan-journalister-oedelaegger-befolkningen/).

<img src="{{ site.baseurl }}/assets/pecha_kucha.jpg" width="28">
*Talk at Pecha Kucha #30 - Nov 26, 2014*

Living and sensing with a magnetic implant is an ongoing exploration. In the last year or so, it has become a much more subconscious and embodied sensation. In retrospect the project on sensory modality has not been interesting study of magnetic sensing, but has also shaped how I think of elusive research topics and studying these in a more subjective or phenomenological manner.
<br/> <br/>
<br/> <br/>

## Q-Learning for a bi-pedal walker

![alt text]({{ site.baseurl }}/assets/walker_1.gif "Walker falling over")
*Initial steps of a walker during simulation*

This project studies how a bipedal body can learn forward movement within a simulated 2D physics environment through a reinforcement learning algorithm. Basically this is having an artificial intelligence teaching itself how to walk without any knowledge of the concept of walking. It only learns through knowledge which is transferred from each generation of walkers. This is still very much a work in progress. Done in collaboration with Sara Daugbjerg and Younes Haroun Bakhti.

[Github](https://github.com/faaip/BipedLearner_RUC)
<br/> <br/>
<br/> <br/>

## Various webcam tools

![alt text]({{ site.baseurl }}/assets/webcampoint_1.jpg "Wrapcity - black/white")
*Selfie using WebcamPointPlotter*

I have created various tools for glitching and manipulating webcam feeds. Initially I was interested in the "honesty" of bit-shifting pixels since the crude manipulation of pixels offered a more _true_ insight to the inner working of pixels and 32-bit integers. This tool can be found on [Github](https://github.com/faaip/WebcamGlitcher) and has later been used by fine artist and good friend Esben Holk in various of his [video works](https://vimeo.com/184868004). Fascinated by the intimate and empowering of elements of using the webcam led me to create WebcamPointPlotter in [C++](https://github.com/faaip/WebcamPointCloud), [p5js](https://github.com/faaip/WebcamPointPlotter_p5js) and [Processing](https://github.com/faaip/WebcamPointPlotter). This programs transforms webcam inputs to 3d-points and the approach now serves as my 'hello world' for new languages.
<br/> <br/>
<br/> <br/>
<br/> <br/>
<br/> <br/>
